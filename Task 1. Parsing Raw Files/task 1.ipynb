{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W4tQ4m19SAe"
   },
   "source": [
    "# **Task 1: Parsing Raw Files**\n",
    "Step 1: Achieve txt files parsing to csv format\n",
    "\n",
    "Step 2: Achieve xlsx file parsing to csv format\n",
    "\n",
    "Step 3: Combine all data together and output task1.csv\n",
    "\n",
    "Step 4: Achieve txt files parsing to json format\n",
    "\n",
    "Step 5: Achieve xlsx file parsing to json format\n",
    "\n",
    "Step 6: Combine all data together and output task1_output.json\n",
    "\n",
    "Step 7: Generate necessary files and report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dXBo25-hGHoI"
   },
   "outputs": [],
   "source": [
    "# ignore FutureWarning, UserWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uyaA5KjzDuOG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYqIT55JJ75C"
   },
   "source": [
    "**Step 1:** Achieve txt files parsing to csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ULY7uJiXSRg"
   },
   "source": [
    "List all files in a directory, filtering the files using a regular expression. This line filters the file list to include only files that match the pattern group123_<number>.txt (for example, group123_1.txt, group123_2.txt). Regular expressions ensure that only these archives are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KS6QryOdAsIc"
   },
   "outputs": [],
   "source": [
    "# input txt files\n",
    "file_dir = os.listdir('/Original_data')\n",
    "files = [f for f in file_dir if re.match(r'group123_\\d+\\.txt', f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UgDQp_LYFYC"
   },
   "source": [
    "creat an empty list, which will hold DataFrames created from the data extracted from the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "08BioMvvBGk-"
   },
   "outputs": [],
   "source": [
    "df_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxpItFatZ4Sq"
   },
   "source": [
    "1. Loop through each filtered text file, open and read the contents of each file, and store it in the data variable. Retrieve records using regular expressions, \"record(.*?)record\" code searches for all text contained within the tag. The re.DOTALL flag allows matching newlines, enabling the capture of multiple lines of records.\n",
    "\n",
    "2. Create three empty lists to store gmap_id, text, and resp values extracted from each record.\n",
    "\n",
    "3. Extract fields from each record, Within each record, the code uses regular expressions to search for and extract gmap_id, text, and resp values. These values are extracted from tags like gmap_id, review, or response.\n",
    "\n",
    "4. Append extracted data to lists, the extracted values are stripped of any leading or trailing space and then added to their respective lists.\n",
    "\n",
    "5. After processing all records in a file, create a DataFrame from the gmap_id, text, and resp lists. Then, append the DataFrame to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cWd4CiRXZk3N"
   },
   "outputs": [],
   "source": [
    "for f_name in files:\n",
    "  f_path = os.path.join('/Original_data', f_name)\n",
    "\n",
    "  with open(f_path, 'r', encoding='utf-8') as file:\n",
    "      data = file.read()\n",
    "\n",
    "  records = re.findall(r'<record>(.*?)</record>', data, re.DOTALL)\n",
    "\n",
    "  gmap_list = []\n",
    "  text_list = []\n",
    "  resp_list = []\n",
    "\n",
    "  for record in records:\n",
    "    gmap_id = re.search(r'<\\s*[gG]map[_IDid]*\\s*>\\s*([^<]+)\\s*<', record)\n",
    "    text = re.search(r'<\\s*(?:[rR]eview|[tT]ext)\\s*>\\s*([^<]+)\\s*<', record)\n",
    "    resp = re.search(r'<\\s*(?:[rR]esp(?:onse)?)\\s*>\\s*([^<]+)\\s*<', record)\n",
    "\n",
    "    gmap_list.append(gmap_id.group(1).strip())\n",
    "    text_list.append(text.group(1).strip())\n",
    "    resp_list.append(resp.group(1).strip())\n",
    "\n",
    "  df = pd.DataFrame({\n",
    "      'gmap_id': gmap_list,\n",
    "      'text': text_list,\n",
    "      'resp': resp_list\n",
    "  })\n",
    "  df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVDLD71S39fn"
   },
   "source": [
    "**Step 2:** Achieve xlsx file parsing to csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wMFMHiGmkHyP"
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('/Original_data/group123.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRh51DoN5OnQ"
   },
   "source": [
    "1. Reads the current sheet into a DataFrame named sheet_df, then, removes any rows in the DataFrame where the gmap_id column is NaN. This ensures that only rows with valid gmap_id values are processed.\n",
    "2. converting text&resp to a string and replacing 'nan' with 'None'.\n",
    "3. This line extracts the gmap_id, text, and resp columns from the cleaned DataFrame and appends them to the df_list.\n",
    "4. Combines all the DataFrames stored in df_list into a single DataFrame called df_concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf48C-RC5CZy",
    "outputId": "7b5475d1-c181-4921-8640-c3f0498fb67e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-093b650650c0>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sheet_df['text'] = sheet_df['text'].astype(str)\n",
      "<ipython-input-8-093b650650c0>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sheet_df['text'] = sheet_df['text'].replace('nan', 'None')\n",
      "<ipython-input-8-093b650650c0>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sheet_df['resp'] = sheet_df['resp'].astype(str)\n",
      "<ipython-input-8-093b650650c0>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sheet_df['resp'] = sheet_df['resp'].replace('nan', 'None')\n"
     ]
    }
   ],
   "source": [
    "for sheet_name in xls.sheet_names:\n",
    "  sheet_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "  sheet_df = sheet_df.dropna(subset = ['gmap_id'])\n",
    "  sheet_df['text'] = sheet_df['text'].astype(str)\n",
    "  sheet_df['text'] = sheet_df['text'].replace('nan', 'None')\n",
    "  sheet_df['resp'] = sheet_df['resp'].astype(str)\n",
    "  sheet_df['resp'] = sheet_df['resp'].replace('nan', 'None')\n",
    "  df_list.append(sheet_df[['gmap_id', 'text', 'resp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df2-NPnu-FPM"
   },
   "source": [
    "1. Groups the combined DataFrame by the gmap_id column. This means that all rows with the same gmap_id are grouped together.\n",
    "\n",
    "2. Aggregates the grouped data with calculations:\n",
    "  A. number of reviews (i.e., the number of rows in each group).\n",
    "  B. number of non-'None' texts in each group.\n",
    "  C. number of non-'None' responses in each group.\n",
    "3. Converts the grouped data back into a DataFrame format with a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Kyp4IDSy9yRJ"
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df_grouped = df_concat.groupby('gmap_id').agg(\n",
    "    review_count=('text', 'size'),\n",
    "    review_text_count=('text', lambda x: (x.str.lower() != 'none').sum()),\n",
    "    response_count=('resp', lambda x: (x.str.lower() != 'none').sum())\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPZ4GIonAM-b"
   },
   "source": [
    "**Step 3:** Combine all data together and output task1_123.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zsMVDukH9zSF"
   },
   "outputs": [],
   "source": [
    "df_grouped.to_csv('/task1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiN1OzJ8ZVTS"
   },
   "source": [
    "# txt files and xlsx file to JSON\n",
    "Step 4: Achieve txt files parsing to json format\n",
    "\n",
    "Step 5: Achieve xlsx file parsing to json format\n",
    "\n",
    "Step 6: Combine all data together and output task1_output.json\n",
    "\n",
    "Step 7: Generate necessary files and report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfO6C_h-fEcZ"
   },
   "source": [
    "Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jZLtBl9VYBTg"
   },
   "outputs": [],
   "source": [
    "# Function to transfer Unix timestamp to UTC\n",
    "def transfer_date_to_utc(review_time):\n",
    "  return datetime.fromtimestamp(int(review_time) / 1000, timezone.utc).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LysuzdiukiWy"
   },
   "source": [
    "  This function cleans a given review by:\n",
    "1. Extracting the English part of the review if it's translated by Google.\n",
    "2. Converting the text to lowercase.\n",
    "3. Removing emojis from the text.\n",
    "4. Ensuring that an empty or irrelevant review is replaced with \"None\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EBgUdt4pYBf-"
   },
   "outputs": [],
   "source": [
    "def review_cleaning(review):\n",
    "  # extract english reviews\n",
    "  english_review_match = re.search(r\"\\(Translated by Google\\)\\s*(.*?)\\s*\\(Original\\)\", review)\n",
    "  english_review = english_review_match.group(1).strip() if english_review_match else review\n",
    "  # ensure text is in a lowercase\n",
    "  lowercase_review = english_review.lower()\n",
    "  emoji_pattern = re.compile(\n",
    "      \"[\"\n",
    "      \"\\U0001F600-\\U0001F64F\"\n",
    "      \"\\U0001F300-\\U0001F5FF\"\n",
    "      \"\\U0001F680-\\U0001F6FF\"\n",
    "      \"\\U0001F1E0-\\U0001F1FF\"\n",
    "      \"\\U00002702-\\U000027B0\"\n",
    "      \"\\U000024C2-\\U0001F251\"\n",
    "      \"]+\", flags=re.UNICODE\n",
    "  )\n",
    "  # remove emojis\n",
    "  lowercase_review = emoji_pattern.sub(r'', lowercase_review).strip()\n",
    "\n",
    "  # There are 2 situations. 1. after removing emojis, review can be empty, we add \"None\". 2. At the beginning it shows \"None\"\n",
    "  return lowercase_review if lowercase_review and lowercase_review != \"none\" else \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tb-6Jz84khVl"
   },
   "source": [
    "This function extracts the dimensions (width and height) from picture URLs provided in a text string and returns them as a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0spLg1BdYBuZ"
   },
   "outputs": [],
   "source": [
    "# Function to get dimensions from picture URLs in txt files\n",
    "def get_dimensions(pics):\n",
    "  dim_list = []\n",
    "  urls = re.findall(r'https?://[^\\'\\]}]+', pics)\n",
    "  for url in urls:\n",
    "      dim_match = re.search(r'w(\\d+)-h(\\d+)', url)\n",
    "      if dim_match:\n",
    "          width, height = dim_match.groups()\n",
    "          dim_list.append([height, width])\n",
    "  return dim_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc_8PbhUlq32"
   },
   "source": [
    "This function cleans the review text, converts the review time to UTC, and extracts relevant information into a structured format. Then, return dictionary containing processed review data with keys for user ID, review time, review rating, cleaned review text, presence of pictures, picture dimensions, and presence of a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vHENTM-orf1"
   },
   "source": [
    "Processing txt files by these steps:\n",
    "1. Extract all records from the file\n",
    "2. Process each record individually\n",
    "3. Extract the user ID from the record\n",
    "4. Convert the review time to UTC\n",
    "5. Extract the review rating and convert it to a float\n",
    "6. Extract and clean the review text\n",
    "7. Extract picture URLs and determine if pictures are present\n",
    "8. Check if there is a response to the review\n",
    "9. Create a dictionary to store the processed review data\n",
    "10. Append the processed review to the list of reviews for the gmap_id\n",
    "11. Update the earliest & latest review date if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NpAO4VrAYB4R"
   },
   "outputs": [],
   "source": [
    "# Processing txt files\n",
    "txt_files = [f for f in file_dir if f.endswith('.txt')]\n",
    "out_dic = {}\n",
    "\n",
    "for f_name in txt_files:\n",
    "  f_path = os.path.join('/Original_data', f_name)\n",
    "  with open(f_path, 'r', encoding='utf-8') as file:\n",
    "      data = file.read()\n",
    "\n",
    "  records = re.findall(r'<record>(.*?)</record>', data, re.DOTALL)\n",
    "\n",
    "  # Extract the gmap_id from the record\n",
    "  for record in records:\n",
    "      gmap_id = re.search(r'<\\s*[gG]map[_IDid]*\\s*>\\s*([^<]+)\\s*<', record).group(1).strip()\n",
    "\n",
    "      if gmap_id not in out_dic:\n",
    "          out_dic[gmap_id] = {\n",
    "              \"reviews\": [],\n",
    "              \"earliest_review_date\": None,\n",
    "              \"latest_review_date\": None\n",
    "          }\n",
    "\n",
    "      user_id_match = re.search(r'<\\s*[uU]ser[_.IDid]*\\s*>\\s*([^<]+)\\s*<', record)\n",
    "      user_id = user_id_match.group(1).strip() if user_id_match else \"None\"\n",
    "\n",
    "      review_time_utc = transfer_date_to_utc(\n",
    "          re.search(r'<\\s*(?:[tT]ime|[dD]ate)\\s*>\\s*([^<]+)\\s*<', record).group(1)).strip()\n",
    "\n",
    "      rating_match = re.search(r'<\\s*(?:[rR]at(?:e|ing))\\s*>\\s*([^<]+)\\s*<', record).group(1)\n",
    "      # If conversion fails, keep the original value\n",
    "      try:\n",
    "          rating = float(rating_match)\n",
    "      except ValueError:\n",
    "          rating = rating_match\n",
    "\n",
    "      review_text_match = re.search(r'<\\s*(?:[rR]eview|[tT]ext)\\s*>\\s*([^<]+)\\s*<', record).group(1).strip()\n",
    "      # if the original content is empty <><> or no review tags -> None\n",
    "      review_text = review_cleaning(review_text_match) if review_text_match else \"None\"\n",
    "\n",
    "      pics_match = re.search(r'<\\s*(?:[pP]ic(?:ture)?s)\\s*>\\s*([^<]+)\\s*<', record).group(1).strip()\n",
    "      pics = pics_match if pics_match else \"None\"\n",
    "\n",
    "      # Mark \"Y\" if pictures are present, otherwise \"N\"\n",
    "      if_pic = \"Y\" if pics.lower() != \"none\" else \"N\"\n",
    "\n",
    "      # Extract picture dimensions if pictures are present\n",
    "      pic_dim = get_dimensions(pics) if if_pic == 'Y' else '[]'\n",
    "\n",
    "      response_match = re.search(r'<\\s*(?:[rR]esp(?:onse)?)\\s*>\\s*([^<]+)\\s*<', record)\n",
    "      if_response = \"Y\" if response_match and response_match.group(1).lower() != \"none\" else \"N\"\n",
    "\n",
    "      review_inner = {\n",
    "          \"user_id\": user_id,\n",
    "          \"time\": review_time_utc,\n",
    "          \"review_rating\": rating,\n",
    "          \"review_text\": review_text,\n",
    "          \"if_pic\": if_pic,\n",
    "          \"pic_dim\": pic_dim,\n",
    "          \"if_response\": if_response\n",
    "      }\n",
    "\n",
    "      out_dic[gmap_id][\"reviews\"].append(review_inner)\n",
    "\n",
    "      # Update earliest review dates\n",
    "      if not out_dic[gmap_id][\"earliest_review_date\"] or review_time_utc < out_dic[gmap_id][\"earliest_review_date\"]:\n",
    "          out_dic[gmap_id][\"earliest_review_date\"] = review_time_utc\n",
    "      # Update latest review dates\n",
    "      if not out_dic[gmap_id][\"latest_review_date\"] or review_time_utc > out_dic[gmap_id][\"latest_review_date\"]:\n",
    "          out_dic[gmap_id][\"latest_review_date\"] = review_time_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a984vG8Vr6fN"
   },
   "source": [
    "Processing xlsx file by these steps:\n",
    "1. Process each sheet in the Excel file\n",
    "2. Combine data from all sheets into a single DataFrame\n",
    "3. Process the combined data by grouping it by gmap_id\n",
    "4. Update the out_dic dictionary with the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HQNem7HuYBy-"
   },
   "outputs": [],
   "source": [
    "# Function to process reviews from xlsx files\n",
    "def process_review(row):\n",
    "  clean_text = review_cleaning(row['text']) if pd.notna(row['text']) else \"None\"\n",
    "  return {\n",
    "      \"user_id\": row['user_id'],\n",
    "      \"time\": transfer_date_to_utc(row['time']),\n",
    "      \"review_rating\": float(row['rating']),\n",
    "      \"review_text\": clean_text,\n",
    "      \"If_pic\": \"Y\" if pd.notna(row['pics']) else \"N\",\n",
    "      \"pic_dim\": [],\n",
    "      \"If_response\": \"Y\" if pd.notna(row['resp']) else \"N\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "s1WZEtOIYB6-"
   },
   "outputs": [],
   "source": [
    "# Processing xlsx files\n",
    "xlsx_files = [f for f in file_dir if f.endswith('.xlsx')]\n",
    "\n",
    "for f_name in xlsx_files:\n",
    "  f_path = os.path.join('/Original_data', f_name)\n",
    "  xls = pd.ExcelFile(f_path)\n",
    "  sheet_names = xls.sheet_names\n",
    "  xls_data = []\n",
    "\n",
    "  for sheet_name in sheet_names:\n",
    "      sheet_data = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "      valid_data = sheet_data.dropna(subset=['gmap_id'])\n",
    "      xls_data.append(valid_data)\n",
    "\n",
    "  # all valid data into one dataframe\n",
    "  combined_data = pd.concat(xls_data, ignore_index=True)\n",
    "\n",
    "  # Apply the process_review function to each row in the group and convert the result to a list\n",
    "  for gmap_id, group in combined_data.groupby('gmap_id'):\n",
    "      reviews = group.apply(process_review, axis=1).tolist()\n",
    "\n",
    "      #convert to UTC\n",
    "      earliest_review_date = transfer_date_to_utc(group['time'].min())\n",
    "      latest_review_date = transfer_date_to_utc(group['time'].max())\n",
    "\n",
    "      if gmap_id not in out_dic:\n",
    "          out_dic[gmap_id] = {\n",
    "              \"gmap_id\": gmap_id,\n",
    "              \"reviews\": reviews,\n",
    "              \"earliest_review_date\": earliest_review_date,\n",
    "              \"latest_review_date\": latest_review_date\n",
    "          }\n",
    "      else:\n",
    "          # update the existing entry if the gmap_id already exists in the dictionary\n",
    "          out_dic[gmap_id][\"reviews\"].extend(reviews)\n",
    "          # Update the earliest & latest review dates\n",
    "          out_dic[gmap_id][\"earliest_review_date\"] = min(out_dic[gmap_id][\"earliest_review_date\"], earliest_review_date)\n",
    "          out_dic[gmap_id][\"latest_review_date\"] = max(out_dic[gmap_id][\"latest_review_date\"], latest_review_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNDmYe8FuzoV"
   },
   "source": [
    "This function reformats a string containing picture dimensions, then, returns the reformatted string with properly formatted picture dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3xYwS55oYCAq"
   },
   "outputs": [],
   "source": [
    "def reformat_pic_dim(dim_str):\n",
    "  pic_dim_match = re.compile(r'\\[\\s*\\[\\s*\"\\d+\"\\s*,\\s*\"\\d+\"\\s*\\](?:\\s*,\\s*\\[\\s*\"\\d+\"\\s*,\\s*\"\\d+\"\\s*\\])*\\s*\\]')\n",
    "  # Replace the matched pattern, ensuring no extra spaces before sub-lists and formatting them properly\n",
    "  pic_dim = pic_dim_match.sub(lambda x: x.group(0).replace('\\n', '').replace(' ', ''), dim_str)\n",
    "  return pic_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "P1f8XsiZYCIE"
   },
   "outputs": [],
   "source": [
    "output_json = json.dumps(out_dic, indent=2, ensure_ascii=False)\n",
    "output_json = reformat_pic_dim(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LuhvF9V9u7m-"
   },
   "outputs": [],
   "source": [
    "with open('/task1_output.json', 'w', encoding='utf-8') as output_file:\n",
    "  output_file.write(output_json)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
